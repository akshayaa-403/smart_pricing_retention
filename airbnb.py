# -*- coding: utf-8 -*-
"""Airbnb.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1P7kdE7h0vDxDsvsUi59nMx_qIMF3YyKB

# Imports and getting data
"""

!pip3 install optuna category_encoders mlflow feast autogluon fastai

# Core data science libraries
import numpy as np
import pandas as pd

# Visualization
import matplotlib.pyplot as plt
import seaborn as sns

# Machine Learning libraries
import xgboost as XGBRegressor
import lightgbm as lgb
import tensorflow as tf
import optuna
import shap
import fastai.tabular.all as fastai
import category_encoders as ce
import mlflow
#import feast
import geopandas as gpd

# Scikit-learn imports
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, TimeSeriesSplit
from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, make_scorer

# Additional ML models
from xgboost import XGBClassifier
from tensorflow import keras
from tensorflow.keras import layers
from autogluon.tabular import TabularPredictor

!wget https://data.insideairbnb.com/united-states/ny/new-york-city/2025-03-01/data/listings.csv.gz
!gunzip listings.csv.gz

!wget https://data.insideairbnb.com/united-states/ny/new-york-city/2025-03-01/data/reviews.csv.gz
!gunzip reviews.csv.gz

!wget https://data.insideairbnb.com/united-states/ny/new-york-city/2025-03-01/data/calendar.csv.gz
!gunzip calendar.csv.gz

!wget https://data.insideairbnb.com/united-states/ny/new-york-city/2025-03-01/visualisations/neighbourhoods.geojson

"""# Data Preprocessing"""

def preprocess_data():
    try:
        print("Starting data preprocessing...")

        # Load the datasets with proper configurations
        print("Loading datasets...")
        listings_df = pd.read_csv('listings.csv', low_memory=False)
        reviews_df = pd.read_csv('reviews.csv', low_memory=False)
        calendar_df = pd.read_csv('calendar.csv', low_memory=False)
        neighborhoods = gpd.read_file('neighbourhoods.geojson')

        # Initial exploration of each dataset
        listings_df = initial_data_exploration(listings_df, "Listings")
        reviews_df = initial_data_exploration(reviews_df, "Reviews")
        calendar_df = initial_data_exploration(calendar_df, "Calendar")

        # Clean price columns in both listings and calendar
        print("\nCleaning price columns...")
        listings_df = clean_price_columns(listings_df)
        calendar_df = clean_price_columns(calendar_df)

        # Handle missing values
        print("\nHandling missing values...")
        listings_df = handle_missing_values(listings_df)
        reviews_df = handle_missing_values(reviews_df)
        calendar_df = handle_missing_values(calendar_df)

        # Create time features for calendar and reviews
        print("\nCreating time features...")
        calendar_df = create_time_features(calendar_df)
        reviews_df = create_time_features(reviews_df)

        print("\nPreprocessing completed successfully!")
        return listings_df, reviews_df, calendar_df, neighborhoods

    except Exception as e:
        print(f"Error in preprocessing pipeline: {str(e)}")
        return None, None, None, None

def clean_price_columns(df):
    """Clean price-related columns by removing '$' and ',' and converting to float"""
    price_columns = ['price', 'weekly_price', 'monthly_price', 'security_deposit', 'cleaning_fee']
    for col in price_columns:
        if col in df.columns:
            # Fixed escape sequence by using raw string r'\$'
            df[col] = df[col].replace({r'\$': '', ',': ''}, regex=True).astype(float)
    return df

def handle_missing_values(df):
    """Handle missing values in the dataset"""
    # First, let's get a summary of missing values
    print("\nMissing Values Summary:")
    missing_summary = df.isnull().sum()[df.isnull().sum() > 0]
    print(missing_summary)

    # Fill numerical columns with median
    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns
    for col in numerical_cols:
        if df[col].isnull().sum() > 0:
            print(f"Filling missing values in {col} with median")
            df[col] = df[col].fillna(df[col].median())

    # Fill categorical columns with mode
    categorical_cols = df.select_dtypes(include=['object']).columns
    for col in categorical_cols:
        if df[col].isnull().sum() > 0:
            print(f"Filling missing values in {col} with mode")
            df[col] = df[col].fillna(df[col].mode().iloc[0])

    return df

def create_time_features(df):
    """Create temporal features from date columns"""
    try:
        if 'date' in df.columns:
            df['date'] = pd.to_datetime(df['date'])
            print("Successfully converted 'date' column to datetime")

            df['month'] = df['date'].dt.month
            df['day_of_week'] = df['date'].dt.dayofweek
            df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)

            # Add season
            df['season'] = df['date'].dt.month.map({
                1: 'Winter', 2: 'Winter', 3: 'Spring',
                4: 'Spring', 5: 'Spring', 6: 'Summer',
                7: 'Summer', 8: 'Summer', 9: 'Fall',
                10: 'Fall', 11: 'Fall', 12: 'Winter'
            })

        return df
    except Exception as e:
        print(f"Error in create_time_features: {str(e)}")
        return df

def initial_data_exploration(df, dataset_name):
    """Perform initial data exploration and print summary statistics"""
    print(f"\n{'='*50}")
    print(f"Dataset: {dataset_name}")
    print(f"{'='*50}")
    print(f"\nShape: {df.shape}")
    print("\nData Types:")
    print(df.dtypes)
    print("\nSample of first few rows:")
    print(df.head())
    print("\nBasic Statistics:")
    print(df.describe())
    return df

# Execute preprocessing pipeline
if __name__ == "__main__":
    print(f"Current Date and Time (UTC): {pd.Timestamp.now(tz='UTC').strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Current User's Login: akshayaa-403")
    listings_df, reviews_df, calendar_df, neighborhoods = preprocess_data()

# Run the preprocessing pipeline
listings_df, reviews_df, calendar_df, neighborhoods = preprocess_data()

# Verify the data was loaded correctly
if listings_df is not None:
    print("\nDataset Shapes:")
    print(f"Listings: {listings_df.shape}")
    print(f"Reviews: {reviews_df.shape}")
    print(f"Calendar: {calendar_df.shape}")

    # Save preprocessed data
    print("\nSaving preprocessed data...")
    listings_df.to_csv('preprocessed_listings.csv', index=False)
    reviews_df.to_csv('preprocessed_reviews.csv', index=False)
    calendar_df.to_csv('preprocessed_calendar.csv', index=False)
    print("Preprocessed data saved successfully!")
else:
    print("Error occurred during preprocessing. Please check the error messages above.")

"""# Feature Engineering"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from datetime import datetime
import json
import ast

class FeatureEngineer:
    def __init__(self, listings_df, reviews_df, calendar_df):
        print(f"Initializing Feature Engineering at: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Current User: akshayaa-403")
        self.listings_df = listings_df.copy()
        self.reviews_df = reviews_df.copy()
        self.calendar_df = calendar_df.copy()
        self.label_encoders = {}

    def clean_amenities_string(self, amenities_str):
        """Safely clean and parse amenities string"""
        try:
            if pd.isna(amenities_str) or amenities_str == '':
                return []
            # Remove any invalid characters and normalize the string
            cleaned_str = amenities_str.replace('"', '"').replace('"', '"')
            cleaned_str = cleaned_str.strip('[]').strip()
            # Split by comma and clean each item
            amenities_list = [item.strip().strip('"\'') for item in cleaned_str.split(',')]
            return [item for item in amenities_list if item]
        except Exception as e:
            print(f"Error cleaning amenities string: {str(e)}")
            return []

    def engineer_listing_features(self):
        """Create advanced features for listings"""
        print("\nEngineering listing features...")

        try:
            # Amenities processing
            print("Processing amenities...")
            self.listings_df['amenities_list'] = self.listings_df['amenities'].apply(self.clean_amenities_string)
            self.listings_df['amenities_count'] = self.listings_df['amenities_list'].str.len()

            # Create essential amenities features
            essential_amenities = ['Wifi', 'Air conditioning', 'Kitchen', 'Heating', 'Washer']
            for amenity in essential_amenities:
                self.listings_df[f'has_{amenity.lower().replace(" ", "_")}'] = self.listings_df['amenities_list'].apply(
                    lambda x: 1 if any(amenity.lower() in item.lower() for item in x) else 0
                )

            # Location features
            print("Creating location-based features...")
            self.listings_df['location_score'] = (
                self.listings_df['review_scores_location'].fillna(0) *
                self.listings_df['review_scores_accuracy'].fillna(0)
            ) / 100

            # Price features
            print("Creating price-related features...")
            self.listings_df['price_per_person'] = (
                self.listings_df['price'].fillna(0) /
                self.listings_df['accommodates'].replace(0, 1)
            )
            self.listings_df['price_per_bathroom'] = (
                self.listings_df['price'].fillna(0) /
                self.listings_df['bathrooms'].fillna(1).replace(0, 1)
            )

            # Host features
            print("Creating host-related features...")
            self.listings_df['host_since'] = pd.to_datetime(self.listings_df['host_since'])
            self.listings_df['host_experience_days'] = (
                pd.Timestamp('2025-05-03') - self.listings_df['host_since']
            ).dt.days.fillna(0)

            # Response features
            self.listings_df['is_superhost'] = self.listings_df['host_is_superhost'].map({'t': 1, 'f': 0, True: 1, False: 0}).fillna(0)
            self.listings_df['instant_bookable'] = self.listings_df['instant_bookable'].map({'t': 1, 'f': 0, True: 1, False: 0}).fillna(0)

            # Review features
            print("Creating review-based features...")
            review_cols = [col for col in self.listings_df.columns if col.startswith('review_scores_')]
            self.listings_df['avg_review_score'] = self.listings_df[review_cols].fillna(0).mean(axis=1)

            # Drop the original amenities columns and temporary columns
            self.listings_df = self.listings_df.drop(['amenities', 'amenities_list'], axis=1, errors='ignore')

            print("Listing features engineering completed successfully!")
            return self.listings_df

        except Exception as e:
            print(f"Error in engineer_listing_features: {str(e)}")
            return self.listings_df

    def engineer_churn_features(self):
        """Create features for churn prediction"""
        print("\nEngineering churn features...")

        try:
            # Convert dates to datetime
            self.reviews_df['date'] = pd.to_datetime(self.reviews_df['date'])

            # Calculate booking frequency metrics
            print("Calculating booking frequency metrics...")
            booking_metrics = self.reviews_df.groupby('listing_id').agg({
                'date': ['count', 'max', 'min'],
                'reviewer_id': 'nunique'
            }).reset_index()

            # Flatten column names
            booking_metrics.columns = [
                'listing_id', 'total_bookings', 'last_booking_date',
                'first_booking_date', 'unique_customers'
            ]

            # Calculate time-based metrics
            current_date = pd.Timestamp('2025-05-03')
            booking_metrics['days_since_last_booking'] = (
                current_date - booking_metrics['last_booking_date']
            ).dt.days

            booking_metrics['booking_timespan'] = (
                booking_metrics['last_booking_date'] - booking_metrics['first_booking_date']
            ).dt.days

            # Handle infinite values
            booking_metrics['booking_frequency'] = booking_metrics['total_bookings'] / (booking_metrics['booking_timespan'] + 1)
            booking_metrics['repeat_customer_rate'] = booking_metrics['unique_customers'] / booking_metrics['total_bookings']

            # Fill NaN values
            booking_metrics = booking_metrics.fillna({
                'booking_frequency': 0,
                'repeat_customer_rate': 0,
                'days_since_last_booking': 365
            })

            # Create churn risk categories
            booking_metrics['churn_risk'] = pd.cut(
                booking_metrics['days_since_last_booking'],
                bins=[0, 30, 90, 180, np.inf],
                labels=['Low', 'Medium', 'High', 'Very High']
            )

            print("Churn features engineering completed successfully!")
            return booking_metrics

        except Exception as e:
            print(f"Error in engineer_churn_features: {str(e)}")
            return pd.DataFrame({'listing_id': self.listings_df['id'].unique()})

    def engineer_calendar_features(self):
        """Create features from calendar data"""
        print("\nEngineering calendar features...")

        try:
            # Convert date to datetime and ensure price is numeric
            self.calendar_df['date'] = pd.to_datetime(self.calendar_df['date'])
            # Fix the invalid escape sequence by using raw string r'\$' instead of '\$'
            self.calendar_df['price'] = pd.to_numeric(self.calendar_df['price'].replace({r'\$': '', ',': ''}, regex=True), errors='coerce')

            # Calculate availability metrics
            availability_metrics = self.calendar_df.groupby('listing_id').agg({
                'available': ['count', lambda x: (x == 't').sum()],
                'price': ['mean', 'std', 'min', 'max']
            }).reset_index()

            # Flatten column names
            availability_metrics.columns = [
                'listing_id', 'total_days', 'available_days',
                'avg_price', 'price_std', 'min_price', 'max_price'
            ]

            # Calculate additional metrics
            availability_metrics['occupancy_rate'] = 1 - (
                availability_metrics['available_days'] / availability_metrics['total_days']
            )

            availability_metrics['price_variance'] = (
                availability_metrics['price_std'] / availability_metrics['avg_price']
            ).fillna(0)

            # Fill NaN values
            availability_metrics = availability_metrics.fillna({
                'avg_price': 0,
                'price_std': 0,
                'min_price': 0,
                'max_price': 0,
                'occupancy_rate': 0,
                'price_variance': 0
            })

            print("Calendar features engineering completed successfully!")
            return availability_metrics

        except Exception as e:
            print(f"Error in engineer_calendar_features: {str(e)}")
            return pd.DataFrame({'listing_id': self.listings_df['id'].unique()})

    def combine_features(self, listing_features, churn_features, calendar_features):
        """Combine all engineered features into a single dataset"""
        print("\nCombining all engineered features...")

        try:
            # Ensure listing_id columns are properly named
            if 'id' in listing_features.columns and 'listing_id' not in listing_features.columns:
                listing_features = listing_features.rename(columns={'id': 'listing_id'})

            # Merge all features on listing_id
            final_dataset = listing_features.merge(
                churn_features, on='listing_id', how='left'
            ).merge(
                calendar_features, on='listing_id', how='left'
            )

            # Fill missing values
            final_dataset = final_dataset.fillna({
                'total_bookings': 0,
                'booking_frequency': 0,
                'churn_risk': 'Very High',
                'occupancy_rate': 0,
                'price_variance': 0
            })

            print("\nFeature engineering completed successfully!")
            print(f"Final dataset shape: {final_dataset.shape}")

            return final_dataset

        except Exception as e:
            print(f"Error in combine_features: {str(e)}")
            return None

# Example usage
if __name__ == "__main__":
    try:
        # Load preprocessed data
        listings_df = pd.read_csv('preprocessed_listings.csv')
        reviews_df = pd.read_csv('preprocessed_reviews.csv')
        calendar_df = pd.read_csv('preprocessed_calendar.csv')

        # Initialize feature engineering
        feature_engineer = FeatureEngineer(listings_df, reviews_df, calendar_df)

        # Generate features
        listing_features = feature_engineer.engineer_listing_features()
        churn_features = feature_engineer.engineer_churn_features()
        calendar_features = feature_engineer.engineer_calendar_features()

        # Combine all features
        final_dataset = feature_engineer.combine_features(
            listing_features, churn_features, calendar_features
        )

        # Save the final dataset
        if final_dataset is not None:
            final_dataset.to_csv('engineered_features.csv', index=False)
            print("\nEngineered features saved successfully!")

            # Print feature statistics
            print("\nFeature Statistics:")
            print(f"Number of numerical features: {final_dataset.select_dtypes(include=['int64', 'float64']).shape[1]}")
            print(f"Number of categorical features: {final_dataset.select_dtypes(include=['object']).shape[1]}")

    except Exception as e:
        print(f"Error in main execution: {str(e)}")

"""# Price Prediction"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import joblib

from sklearn.model_selection import train_test_split, cross_val_score, KFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor

class PricePredictionModel:
    def __init__(self):
        print(f"Initializing Price Prediction Model at: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Current User: akshayaa-403")
        self.models = {}
        self.preprocessor = None
        self.feature_importance = {}
        self.best_model = None
        self.best_model_name = None
        self.best_score = float('-inf')

    def load_and_prepare_data(self, data_path='engineered_features.csv'):
      """Load and prepare data for price prediction"""
      print("\nLoading and preparing data...")

      try:
        # Load the engineered features
        df = pd.read_csv(data_path)

        # Remove any rows where price is null or 0
        df = df[df['price'].notna() & (df['price'] > 0)]

        # Convert percentage strings to floats
        percentage_columns = [
            'host_response_rate',
            'host_acceptance_rate'
        ]
        for col in percentage_columns:
            if col in df.columns:
                df[col] = df[col].str.rstrip('%').astype('float') / 100.0

        # Convert date columns to datetime and extract useful features
        date_columns = [
            'last_scraped',
            'calendar_last_scraped'
        ]
        for col in date_columns:
            if col in df.columns:
                df[col] = pd.to_datetime(df[col])
                df[f'{col}_month'] = df[col].dt.month
                df[f'{col}_day'] = df[col].dt.day
                df = df.drop(columns=[col])  # Drop the original date column

        # Convert boolean strings to integers
        boolean_columns = [
            'host_is_superhost',
            'host_has_profile_pic',
            'has_availability'
        ]
        for col in boolean_columns:
            if col in df.columns:
                df[col] = df[col].map({'t': 1, 'f': 0})

        # Identify numeric and categorical columns properly
        exclude_columns = [
            'price', 'listing_id', 'last_booking_date', 'first_booking_date',
            'churn_risk', 'host_since', 'host_url', 'host_name', 'host_location',
            'host_response_time', 'host_thumbnail_url', 'host_picture_url',
            'host_neighbourhood', 'host_verifications', 'host_identity_verified',
            'neighbourhood', 'neighbourhood_cleansed', 'neighbourhood_group_cleansed',
            'property_type', 'room_type', 'bed_type', 'calendar_updated',
            'first_review', 'last_review', 'license', 'jurisdiction_names',
            'source', 'bathrooms_text'  # Adding these to exclude list
        ]

        # Select features
        numeric_features = []
        categorical_features = []

        for col in df.columns:
            if col in exclude_columns:
                continue

            # Check if column is numeric
            if pd.api.types.is_numeric_dtype(df[col]):
                numeric_features.append(col)
            elif df[col].dtype == 'object':
                # Only include categorical columns with reasonable number of unique values
                if df[col].nunique() < 50:  # Adjust this threshold as needed
                    categorical_features.append(col)

        print("\nFeature Selection Summary:")
        print(f"Number of numeric features: {len(numeric_features)}")
        print(f"Number of categorical features: {len(categorical_features)}")

        # Create preprocessor
        self.preprocessor = ColumnTransformer(
            transformers=[
                ('num', RobustScaler(), numeric_features),
                ('cat', 'passthrough', categorical_features)
            ],
            remainder='drop'  # Drop any columns not specified
        )

        # Prepare X and y
        X = df[numeric_features + categorical_features]
        y = np.log1p(df['price'])  # Log transform the target variable

        # Print feature lists
        print("\nNumeric Features:")
        print(numeric_features)
        print("\nCategorical Features:")
        print(categorical_features)

        # Print sample of selected features
        print("\nSample of selected features:")
        print(X.head())

        # Split the data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        print(f"\nTraining set shape: {X_train.shape}")
        print(f"Test set shape: {X_test.shape}")

        return X_train, X_test, y_train, y_test, numeric_features, categorical_features

      except Exception as e:
        print(f"Error in load_and_prepare_data: {str(e)}")
        raise

    def initialize_models(self):
      """Initialize all models with default parameters"""
      self.models = {
          'Random Forest': Pipeline([
              ('preprocessor', self.preprocessor),
              ('regressor', RandomForestRegressor(
                  n_estimators=100,
                  max_depth=15,
                  min_samples_split=5,
                  min_samples_leaf=2,
                  random_state=42
            ))
        ]),
          'XGBoost': Pipeline([
              ('preprocessor', self.preprocessor),
              ('regressor', XGBRegressor(
                  n_estimators=100,
                  max_depth=7,
                  learning_rate=0.1,
                  random_state=42
            ))
        ]),
          'LightGBM': Pipeline([
              ('preprocessor', self.preprocessor),
              ('regressor', LGBMRegressor(
                  n_estimators=100,
                  max_depth=7,
                  learning_rate=0.1,
                  random_state=42
            ))
        ])
    }

    # Add this new method to help with debugging
    def print_feature_summary(self, X):
      """Print summary statistics for each feature"""
      print("\nFeature Summary:")
      for column in X.columns:
          print(f"\nColumn: {column}")
          print(f"Type: {X[column].dtype}")
          print(f"Number of unique values: {X[column].nunique()}")
          print(f"Number of null values: {X[column].isnull().sum()}")
          if X[column].dtype in ['int64', 'float64']:
              print(f"Min: {X[column].min()}")
              print(f"Max: {X[column].max()}")
              print(f"Mean: {X[column].mean()}")

    def train_and_evaluate(self, X_train, X_test, y_train, y_test):
      """Train and evaluate all models"""
      print("\nTraining and evaluating models...")

      results = {}
      for name, model in self.models.items():
        try:
            print(f"\nTraining {name}...")

            # Train the model
            model.fit(X_train, y_train)

            # Make predictions
            y_pred = model.predict(X_test)

            # Transform predictions back to original scale
            y_test_orig = np.expm1(y_test)
            y_pred_orig = np.expm1(y_pred)

            # Calculate metrics
            mse = mean_squared_error(y_test_orig, y_pred_orig)
            rmse = np.sqrt(mse)
            mae = mean_absolute_error(y_test_orig, y_pred_orig)
            r2 = r2_score(y_test_orig, y_pred_orig)

            # Calculate MAPE
            mape = np.mean(np.abs((y_test_orig - y_pred_orig) / y_test_orig)) * 100

            # Store results
            results[name] = {
                'RMSE': rmse,
                'MAE': mae,
                'R2': r2,
                'MAPE': mape
            }

            # Update best model
            if r2 > self.best_score:
                self.best_score = r2
                self.best_model = model
                self.best_model_name = name

            # Calculate feature importance if available
            if hasattr(model.named_steps['regressor'], 'feature_importances_'):
                self.feature_importance[name] = model.named_steps['regressor'].feature_importances_

        except Exception as e:
            print(f"Error training {name}: {str(e)}")
            continue

      return results

    def cross_validate_best_model(self, X, y, cv=5):
        """Perform cross-validation on the best model"""
        print(f"\nPerforming {cv}-fold cross-validation on {self.best_model_name}...")

        cv_scores = cross_val_score(
            self.best_model, X, y,
            cv=KFold(n_splits=cv, shuffle=True, random_state=42),
            scoring='r2'
        )

        print(f"Cross-validation R² scores: {cv_scores}")
        print(f"Mean R² score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

        return cv_scores

    def plot_feature_importance(self, feature_names):
        """Plot feature importance for the best model"""
        if self.best_model_name in self.feature_importance:
            importance = self.feature_importance[self.best_model_name]
            feature_importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importance
            })

            # Sort by importance and get top 20 features
            feature_importance_df = feature_importance_df.sort_values(
                'importance', ascending=False
            ).head(20)

            plt.figure(figsize=(12, 6))
            sns.barplot(x='importance', y='feature', data=feature_importance_df)
            plt.title(f'Top 20 Feature Importance - {self.best_model_name}')
            plt.tight_layout()
            plt.show()

    def save_model(self, filename='best_price_prediction_model.joblib'):
        """Save the best model to disk"""
        if self.best_model is not None:
            joblib.dump(self.best_model, filename)
            print(f"\nBest model ({self.best_model_name}) saved as {filename}")

def plot_results(results):
    """Plot comparison of model performances"""
    metrics = ['RMSE', 'MAE', 'R2']
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))

    for i, metric in enumerate(metrics):
        values = [results[model][metric] for model in results]
        sns.barplot(x=list(results.keys()), y=values, ax=axes[i])
        axes[i].set_title(metric)
        axes[i].tick_params(axis='x', rotation=45)

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    try:
        # Initialize price prediction model
        price_model = PricePredictionModel()

        # Load and prepare data
        X_train, X_test, y_train, y_test, numeric_features, categorical_features = (
            price_model.load_and_prepare_data()
        )

        # Initialize models
        price_model.initialize_models()

        # Train and evaluate models
        results = price_model.train_and_evaluate(X_train, X_test, y_train, y_test)

        # Print results
        print("\nModel Performance Results:")
        for model_name, metrics in results.items():
            print(f"\n{model_name}:")
            for metric_name, value in metrics.items():
                print(f"{metric_name}: {value:.4f}")

        # Plot results
        plot_results(results)

        # Perform cross-validation on best model
        cv_scores = price_model.cross_validate_best_model(
            pd.concat([X_train, X_test]),
            pd.concat([y_train, y_test])
        )

        # Plot feature importance
        price_model.plot_feature_importance(numeric_features + categorical_features)

        # Save the best model
        price_model.save_model()

    except Exception as e:
        print(f"Error in main execution: {str(e)}")

"""# Churn Prediction"""

import pandas as pd
import numpy as np
from datetime import datetime
import joblib
import traceback
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    precision_recall_curve, average_precision_score
)
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
import seaborn as sns
import matplotlib.pyplot as plt
from imblearn.over_sampling import SMOTE

class ChurnPredictionModel:
    def __init__(self):
        """Initialize the ChurnPredictionModel with current timestamp and user"""
        self.current_time = "2025-05-17 17:57:58"
        self.current_user = "akshayaa-403"
        print(f"Current Date and Time (UTC): {self.current_time}")
        print(f"Current User's Login: {self.current_user}")

        self.models = {}
        self.preprocessor = None
        self.feature_importance = {}
        self.best_model = None
        self.best_model_name = None
        self.best_score = float('-inf')
        self.feature_names = None

    def load_and_prepare_data(self, data_path='engineered_features.csv'):
      """Load and prepare data with balanced classes and robust feature selection"""
      print(f"\nLoading and preparing data...")

      try:
        # Load the engineered features
        df = pd.read_csv(data_path)

        # Create fundamental business-focused features
        df['beds_per_accommodates'] = df['beds'] / df['accommodates'].replace(0, 1)
        df['bathrooms_per_beds'] = df['bathrooms'] / df['beds'].replace(0, 1)
        df['amenities_density'] = df['amenities_count'] / (df['accommodates'].replace(0, 1))

        # Calculate thresholds with more balanced percentiles
        price_threshold = df['price'].quantile(0.55)
        min_nights_threshold = df['minimum_nights'].quantile(0.55)
        amenities_threshold = df['amenities_count'].quantile(0.45)
        beds_ratio_threshold = df['beds_per_accommodates'].quantile(0.45)
        bathroom_ratio_threshold = df['bathrooms_per_beds'].quantile(0.45)

        print("\nBusiness Metric Thresholds:")
        print(f"Price (55th percentile): {price_threshold:.2f}")
        print(f"Minimum Nights (55th percentile): {min_nights_threshold:.2f}")
        print(f"Amenities Count (45th percentile): {amenities_threshold:.2f}")
        print(f"Beds per Accommodates (45th percentile): {beds_ratio_threshold:.2f}")
        print(f"Bathrooms per Beds (45th percentile): {bathroom_ratio_threshold:.2f}")

        # Define churn with more balanced rules
        df['is_churned'] = 0
        churn_conditions = (
            # Price efficiency
            ((df['price'] > price_threshold) &
             (df['beds_per_accommodates'] < beds_ratio_threshold)) |
            # Booking restrictions
            ((df['minimum_nights'] > min_nights_threshold) &
             (df['amenities_count'] < amenities_threshold)) |
            # Facility issues
            ((df['bathrooms_per_beds'] < bathroom_ratio_threshold) &
             (df['amenities_density'] < df['amenities_density'].quantile(0.45)))
        )
        df.loc[churn_conditions, 'is_churned'] = 1

        # Keep only essential features
        keep_columns = [
            # Core metrics
            'accommodates',
            'bathrooms',
            'bedrooms',
            'beds',
            'price',
            'minimum_nights',
            'maximum_nights',
            'amenities_count',

            # Basic amenities
            'has_wifi',
            'has_air_conditioning',
            'has_kitchen',
            'has_heating',
            'has_washer',

            # Location
            'latitude',
            'longitude',

            # Booking configuration
            'instant_bookable',

            # Derived ratios
            'beds_per_accommodates',
            'bathrooms_per_beds',
            'amenities_density',

            # Target
            'is_churned'
        ]

        df = df[keep_columns]

        # Split features and target
        X = df.drop('is_churned', axis=1)
        y = df['is_churned']

        print("\nInitial Class Distribution:")
        print(y.value_counts(normalize=True))

        # Apply SMOTE with exact 50-50 balance
        smote = SMOTE(random_state=42, sampling_strategy=1.0)
        X_resampled, y_resampled = smote.fit_resample(X, y)

        print("\nPost-SMOTE Class Distribution:")
        print(pd.Series(y_resampled).value_counts(normalize=True))

        # Create preprocessor
        numeric_features = X.select_dtypes(include=['float64', 'int64']).columns.tolist()
        self.preprocessor = ColumnTransformer(
            transformers=[
                ('num', RobustScaler(quantile_range=(5, 95)), numeric_features)
            ],
            remainder='passthrough'
        )

        # Split with stratification
        X_train, X_test, y_train, y_test = train_test_split(
            X_resampled, y_resampled,
            test_size=0.2,
            random_state=42,
            stratify=y_resampled
        )

        self.feature_names = X.columns.tolist()

        return X_train, X_test, y_train, y_test, numeric_features, []

      except Exception as e:
        print(f"Error in load_and_prepare_data: {str(e)}")
        raise

    def initialize_models(self):
      """Initialize models with balanced parameters to prevent overfitting"""
      self.models = {
        'Random Forest': Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', RandomForestClassifier(
                n_estimators=100,
                max_depth=8,
                min_samples_split=20,
                min_samples_leaf=10,
                max_features='sqrt',
                random_state=42,
                class_weight='balanced',
                n_jobs=-1,
                bootstrap=True,
                max_samples=0.7          # Added subsampling
            ))
        ]),
        'XGBoost': Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', XGBClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.7,
                colsample_bytree=0.7,
                min_child_weight=3,
                random_state=42,
                scale_pos_weight=1,      # Already balanced by SMOTE
                reg_lambda=1.5,
                reg_alpha=0.5,
                gamma=0.1,
                n_jobs=-1
            ))
        ]),
        'LightGBM': Pipeline([
            ('preprocessor', self.preprocessor),
            ('classifier', LGBMClassifier(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                subsample=0.7,
                colsample_bytree=0.7,
                min_child_samples=20,
                random_state=42,
                reg_lambda=1.5,
                reg_alpha=0.5,
                n_jobs=-1,
                num_leaves=31,
                feature_fraction=0.7,
                bagging_fraction=0.7,
                bagging_freq=5,
                min_split_gain=0.1,
                verbose=-1
            ))
        ])
    }
    def train_and_evaluate(self, X_train, X_test, y_train, y_test):
        """Train and evaluate all models with enhanced metrics"""
        print("\nTraining and evaluating models...")

        results = {}
        for name, model in self.models.items():
            try:
                print(f"\nTraining {name}...")

                # Train the model
                model.fit(X_train, y_train)

                # Make predictions
                y_pred = model.predict(X_test)
                y_pred_proba = model.predict_proba(X_test)[:, 1]

                # Calculate metrics
                auc_roc = roc_auc_score(y_test, y_pred_proba)
                avg_precision = average_precision_score(y_test, y_pred_proba)
                classification_rep = classification_report(y_test, y_pred)
                conf_matrix = confusion_matrix(y_test, y_pred)

                # Calculate precision-recall curve
                precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)

                # Store results
                results[name] = {
                    'classification_report': classification_rep,
                    'confusion_matrix': conf_matrix,
                    'auc_roc': auc_roc,
                    'average_precision': avg_precision,
                    'precision_recall_curve': {
                        'precision': precision,
                        'recall': recall
                    }
                }

                # Update best model if current one is better
                if auc_roc > self.best_score:
                    self.best_score = auc_roc
                    self.best_model = model
                    self.best_model_name = name

                # Calculate feature importance
                if hasattr(model.named_steps['classifier'], 'feature_importances_'):
                    self.feature_importance[name] = model.named_steps['classifier'].feature_importances_

                print(f"\n{name} Results:")
                print(f"AUC-ROC Score: {auc_roc:.4f}")
                print(f"Average Precision Score: {avg_precision:.4f}")
                print("\nClassification Report:")
                print(classification_rep)
                print("\nConfusion Matrix:")
                print(conf_matrix)

                # Plot precision-recall curve
                plt.figure(figsize=(8, 6))
                plt.plot(recall, precision, label=f'{name} (AP={avg_precision:.3f})')
                plt.xlabel('Recall')
                plt.ylabel('Precision')
                plt.title(f'Precision-Recall Curve - {name}')
                plt.legend()
                plt.grid(True)
                plt.show()

            except Exception as e:
                print(f"Error training {name}: {str(e)}")
                continue

        return results

    def cross_validate_best_model(self, X, y, cv=5):
        """Perform cross-validation on the best model"""
        if self.best_model is None:
            raise ValueError("No best model selected. Run train_and_evaluate first.")

        print(f"\nPerforming {cv}-fold cross-validation on {self.best_model_name}...")

        cv_scores = cross_val_score(
            self.best_model, X, y,
            cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=42),
            scoring='roc_auc'
        )

        print(f"Cross-validation AUC-ROC scores: {cv_scores}")
        print(f"Mean AUC-ROC score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})")

        return cv_scores

    def plot_feature_importance(self):
        """Plot feature importance with enhanced visualization"""
        if self.best_model_name not in self.feature_importance:
            print(f"No feature importance available for {self.best_model_name}")
            return

        try:
            importance = self.feature_importance[self.best_model_name]

            # Create DataFrame for feature importance
            feature_importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': importance
            })

            # Sort by importance and get top 20 features
            feature_importance_df = feature_importance_df.sort_values(
                'importance', ascending=False
            ).head(20)

            # Create the plot
            plt.figure(figsize=(12, 8))
            sns.barplot(
                x='importance',
                y='feature',
                data=feature_importance_df,
                palette='viridis'
            )
            plt.title(f'Top 20 Most Important Features - {self.best_model_name}')
            plt.xlabel('Importance')
            plt.ylabel('Feature')
            plt.tight_layout()
            plt.show()

            print("\nTop 20 Most Important Features:")
            for idx, row in feature_importance_df.iterrows():
                print(f"- {row['feature']}: {row['importance']:.4f}")

        except Exception as e:
            print(f"Error plotting feature importance: {str(e)}")
            traceback.print_exc()

    def save_model(self, filename='best_churn_prediction_model.joblib'):
        """Save the best model with enhanced metadata"""
        if self.best_model is None:
            raise ValueError("No best model to save. Run train_and_evaluate first.")

        metadata = {
            'timestamp': "2025-05-17 18:03:00",  # Updated timestamp
            'user': "akshayaa-403",
            'model_name': self.best_model_name,
            'auc_roc_score': self.best_score,
            'features': self.feature_names,
            'model_parameters': self.best_model.named_steps['classifier'].get_params(),
            'preprocessing_steps': str(self.preprocessor),
            'feature_importance': self.feature_importance.get(self.best_model_name, None)
        }

        model_data = {
            'model': self.best_model,
            'metadata': metadata
        }

        joblib.dump(model_data, filename)
        print(f"\nBest model ({self.best_model_name}) saved as {filename}")
        print("Model metadata:")
        for key, value in metadata.items():
            if key not in ['features', 'model_parameters', 'preprocessing_steps', 'feature_importance']:
                print(f"- {key}: {value}")

def plot_confusion_matrices(results):
    """Plot confusion matrices for all models"""
    if not results:
        print("No models were successfully trained.")
        return

    n_models = len(results)
    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))

    if n_models == 1:
        axes = [axes]

    for i, (name, metrics) in enumerate(results.items()):
        conf_matrix = metrics['confusion_matrix']
        sns.heatmap(
            conf_matrix,
            annot=True,
            fmt='d',
            ax=axes[i],
            cmap='YlOrRd'
        )
        axes[i].set_title(f'{name} Confusion Matrix')
        axes[i].set_xlabel('Predicted')
        axes[i].set_ylabel('Actual')

    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    try:
        # Initialize model
        churn_model = ChurnPredictionModel()

        # Load and prepare data
        X_train, X_test, y_train, y_test, numeric_features, categorical_features = (
            churn_model.load_and_prepare_data()
        )

        # Initialize models
        churn_model.initialize_models()

        # Train and evaluate models
        results = churn_model.train_and_evaluate(X_train, X_test, y_train, y_test)

        # Plot confusion matrices
        plot_confusion_matrices(results)

        # Perform cross-validation on best model
        cv_scores = churn_model.cross_validate_best_model(
            pd.concat([X_train, X_test]),
            pd.concat([y_train, y_test])
        )

        # Plot feature importance
        churn_model.plot_feature_importance()

        # Save the best model
        churn_model.save_model()

    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        traceback.print_exc()

"""# Model Evaluation

# Evaluation Metrics
"""